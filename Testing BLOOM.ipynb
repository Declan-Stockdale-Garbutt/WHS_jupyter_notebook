{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97771010",
   "metadata": {},
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "# Download the model (350ish GB)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-7b1\")\n",
    "model = AutoModel.from_pretrained(\"bigscience/bloom-7b1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cdd072",
   "metadata": {},
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import time\n",
    "\n",
    "\n",
    "s = time.time()\n",
    "pipe = pipeline(model=\"bigscience/bloom-7b1\", torch_dtype=torch.bfloat16)\n",
    "print(f\"Time to load model: {time.time()-s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328da0d3",
   "metadata": {},
   "source": [
    "# src: https://stackoverflow.com/questions/16816013/is-it-possible-to-print-using-different-colors-in-ipythons-notebook\n",
    "from IPython.display import HTML as html_print\n",
    "\n",
    "def cstr(s, color='black'):\n",
    "    #return \"<text style=color:{}>{}</text>\".format(color, s)\n",
    "    return \"<text style=color:{}>{}</text>\".format(color, s.replace('\\n', '<br>'))\n",
    "\n",
    "def cstr_with_newlines(s, color='black'):\n",
    "    return \"<text style=color:{}>{}</text>\".format(color, s.replace('\\n', '<br>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5de5d",
   "metadata": {},
   "source": [
    "def local_inf(prompt, temperature=0.7, top_p=None, max_new_tokens=32, repetition_penalty=None, do_sample=False, num_return_sequences=1):  \n",
    "    response = pipe(f\"{prompt}\", \n",
    "                    temperature = temperature, # 0 to 1\n",
    "                    top_p = top_p, # None, 0-1\n",
    "                    max_new_tokens = max_new_tokens, # up to 2047 theoretically\n",
    "                    return_full_text = False, # include prompt or not.\n",
    "                    repetition_penalty = repetition_penalty, # None, 0-100 (penalty for repeat tokens.\n",
    "                    do_sample = do_sample, # True: use sampling, False: Greedy decoding.\n",
    "                    num_return_sequences = num_return_sequences\n",
    "                    )\n",
    "    return html_print(cstr(prompt, color='#f1f1c7') + cstr(response[0]['generated_text'], color='#a1d8eb')), response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341333cb",
   "metadata": {},
   "source": [
    "s = time.time()\n",
    "\n",
    "\n",
    "\n",
    "inp = \"\"\"# Use OpenCV in Python\"\"\"\n",
    "color_resp, resp = local_inf(inp, max_new_tokens=64)\n",
    "color_resp\n",
    "\n",
    "print(f\"Time to perform inference: {time.time()-s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74273f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BloomForCausalLM\n",
    "from transformers import BloomTokenizerFast\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d0dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120bd9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_length = 900\n",
    "\n",
    "prompt = \"\"\"\n",
    "OBJECTIVES: This study sought to assess the association between long working \n",
    "hours, psychosocial safety climate (PSC), work engagement (WE) and new major \n",
    "depression symptoms emerging over the next 12 months. PSC is the work climate \n",
    "supporting workplace psychological health.\n",
    "SETTING: Australian prospective cohort population data from the states of New \n",
    "South Wales, Western Australia and South Australia.\n",
    "PARTICIPANTS: At Time 1, there were 3921 respondents in the sample. \n",
    "Self-employed, casual temporary, unclassified, those with working hours <35 (37% \n",
    "of 2850) and participants with major depression symptoms at Time 1 (6.7% of \n",
    "1782) were removed. The final sample was a population-based cohort of 1084 \n",
    "full-time Australian employees.\n",
    "PRIMARY AND SECONDARY OUTCOME MEASURES: The planned and measured outcomes were \n",
    "new cases of major depression symptoms.\n",
    "RESULTS: Long working hours were not significantly related to new cases of major \n",
    "depression symptoms; however, when mild cases were removed, the 41-48 and \n",
    "≥55 long working hour categories were positively related to major depression \n",
    "symptoms. Low PSC was associated with a threefold increase in risk for new major \n",
    "depression symptoms. PSC was not related to long working hours, and long working \n",
    "hours did not mediate the relationship between PSC and new cases of major \n",
    "depression symptoms. The inverse relationship between PSC and major depression \n",
    "symptoms was stronger for males than females. Additional analyses identified \n",
    "that WE was positively related to long working hours. Long working hours (41-48 \n",
    "and ≥55 hours) mediated a positive relationship between WE and major depression \n",
    "symptoms when mild cases of major depression were removed.\n",
    "CONCLUSION: The results suggest that low workplace PSC and potentially long \n",
    "working hours (41-48; ≥55 hours/week) increase the risk of new major depression \n",
    "symptoms. Furthermore, high WE may increase long working hours and subsequent \n",
    "major depression symptoms.\n",
    "\n",
    "**Questions:**\n",
    "1. Who are the authors?\n",
    "2. Is the sentiment postivie or negative?\n",
    "3. What are the key ideas?\n",
    "4. What topics are mentioned?\n",
    "5. What are the main point?\n",
    "6. What are the key words?\n",
    "\n",
    "**Answers:**\n",
    "1. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9861de8b",
   "metadata": {},
   "source": [
    "# 350M\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "current = time.time()\n",
    "\n",
    "model_350M = BloomForCausalLM.from_pretrained(\"bigscience/bloom-350m\") # 7b1\n",
    "tokenizer_350M = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-350m\") # 7b1 # 1b3\n",
    "\n",
    "print(f\"Time to load model: {time.time()-current}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98286f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = tokenizer_350M(prompt, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# Beam Search\n",
    "current = time.time()\n",
    "\n",
    "output = (tokenizer_350M.decode(model_350M.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       num_beams=2, \n",
    "                       no_repeat_ngram_size=2,\n",
    "                       early_stopping=True\n",
    "                      )[0]))\n",
    "\n",
    "output = (output[(prompt.find('**Questions:**')):])\n",
    "print(output)\n",
    "print(f\"Time to Beam search: {time.time()-current}\")\n",
    "\n",
    "output_text.append('350M beam')\n",
    "output_text.append('\\n')\n",
    "output_text.append(output)\n",
    "output_text.append('Time to Beam search:')\n",
    "output_text.append(time.time()-current)\n",
    "output_text.append('\\n')\n",
    "\n",
    "\n",
    "# Sampling Top-k + Top-p\n",
    "current = time.time()\n",
    "\n",
    "output = (tokenizer_350M.decode(model_350M.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.9\n",
    "                      )[0]))\n",
    "output = (output[(prompt.find('**Questions:**')):])\n",
    "print(output)\n",
    "\n",
    "print(f\"Time to Top-k + Top-p search: {time.time()-current}\")\n",
    "\n",
    "output_text.append('350M topk')\n",
    "output_text.append('\\n')\n",
    "output_text.append(output)\n",
    "output_text.append('Time to topk search:')\n",
    "output_text.append(time.time()-current)\n",
    "output_text.append('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219495fb",
   "metadata": {},
   "source": [
    "# 560M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86735f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "current = time.time()\n",
    "\n",
    "model_560M = BloomForCausalLM.from_pretrained(\"bigscience/bloom-560m\") # 7b1\n",
    "tokenizer_560M = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-560m\") # 7b1 # 1b3\n",
    "\n",
    "print(f\"Time to load model: {time.time()-current}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef58f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer_560M(prompt, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "\n",
    "# Beam Search\n",
    "current = time.time()\n",
    "\n",
    "output = (tokenizer_560M.decode(model_560M.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       num_beams=2, \n",
    "                       no_repeat_ngram_size=2,\n",
    "                       early_stopping=True\n",
    "                      )[0]))\n",
    "output = (output[(prompt.find('**Questions:**')):])\n",
    "print(output)\n",
    "print(f\"Time to Beam search: {time.time()-current}\")\n",
    "\n",
    "output_text.append('560M beam')\n",
    "output_text.append('\\n')\n",
    "output_text.append(output)\n",
    "output_text.append('Time to beam search:')\n",
    "output_text.append(time.time()-current)\n",
    "output_text.append('\\n')\n",
    "# Sampling Top-k + Top-p\n",
    "current = time.time()\n",
    "\n",
    "output = (tokenizer_560M.decode(model_560M.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.9\n",
    "                      )[0]))\n",
    "output = (output[(prompt.find('**Questions:**')):])\n",
    "print(output)\n",
    "print(f\"Time to Top-k + Top-p search: {time.time()-current}\")\n",
    "\n",
    "output_text.append('560M topk')\n",
    "output_text.append('\\n')\n",
    "output_text.append(output)\n",
    "output_text.append('Time to topk search:')\n",
    "output_text.append(time.time()-current)\n",
    "output_text.append('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c93206",
   "metadata": {},
   "source": [
    "## 760M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c94c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "current = time.time()\n",
    "\n",
    "model_760M = BloomForCausalLM.from_pretrained(\"bigscience/bloom-760m\") # 7b1\n",
    "tokenizer_760M = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-760m\") # 7b1 # 1b3\n",
    "\n",
    "print(f\"Time to load model: {time.time()-current}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26fb136",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer_760M(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Beam Search\n",
    "current = time.time()\n",
    "\n",
    "output = (tokenizer_760M.decode(model_760M.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       num_beams=2, \n",
    "                       no_repeat_ngram_size=2,\n",
    "                       early_stopping=True\n",
    "                      )[0]))\n",
    "output = (output[(prompt.find('**Questions:**')):])\n",
    "print(output)\n",
    "print(f\"Time to Beam search: {time.time()-current}\")\n",
    "\n",
    "output_text.append('760m beam')\n",
    "output_text.append('\\n')\n",
    "output_text.append(output)\n",
    "output_text.append('Time to beam search:')\n",
    "output_text.append(time.time()-current)\n",
    "output_text.append('\\n')\n",
    "\n",
    "# Sampling Top-k + Top-p\n",
    "current = time.time()\n",
    "\n",
    "output = (tokenizer_760M.decode(model_760M.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.9\n",
    "                      )[0]))\n",
    "output = (output[(prompt.find('**Questions:**')):])\n",
    "print(output)\n",
    "print(f\"Time to Top-k + Top-p search: {time.time()-current}\")\n",
    "\n",
    "output_text.append('760M topk')\n",
    "output_text.append('\\n')\n",
    "output_text.append(output)\n",
    "output_text.append('Time to topk search:')\n",
    "output_text.append(time.time()-current)\n",
    "output_text.append('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9570a8a",
   "metadata": {},
   "source": [
    "# 1b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6884062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current = time.time()\n",
    "\n",
    "model_1b3 = BloomForCausalLM.from_pretrained(\"bigscience/bloom-1b3\") # 7b1\n",
    "tokenizer_1b3 = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-1b3\") # 7b1 # 1b3\n",
    "\n",
    "print(f\"Time to load model: {time.time()-current}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c20699",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer_1b3(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Beam Search\n",
    "current = time.time()\n",
    "\n",
    "output = (tokenizer_1b3.decode(model_1b3.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       num_beams=2, \n",
    "                       no_repeat_ngram_size=2,\n",
    "                       early_stopping=True\n",
    "                      )[0]))\n",
    "output = (output[(prompt.find('**Questions:**')):])\n",
    "print(output)\n",
    "print(f\"Time to Beam search: {time.time()-current}\")\n",
    "\n",
    "output_text.append('1b3 beam')\n",
    "output_text.append('\\n')\n",
    "output_text.append(output)\n",
    "output_text.append('Time to beam search:')\n",
    "output_text.append(time.time()-current)\n",
    "output_text.append('\\n')\n",
    "\n",
    "# Sampling Top-k + Top-p\n",
    "current = time.time()\n",
    "\n",
    "output = (tokenizer_1b3.decode(model_1b3.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.9\n",
    "                      )[0]))\n",
    "output = (output[(prompt.find('**Questions:**')):])\n",
    "print(output)\n",
    "print(f\"Time to Top-k + Top-p search: {time.time()-current}\")\n",
    "\n",
    "output_text.append('1b3 topk')\n",
    "output_text.append('\\n')\n",
    "output_text.append(output)\n",
    "output_text.append('Time to topk search:')\n",
    "output_text.append(time.time()-current)\n",
    "output_text.append('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0928c83",
   "metadata": {},
   "source": [
    "# 2b5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2318508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "current = time.time()\n",
    "\n",
    "model_2b5 = BloomForCausalLM.from_pretrained(\"bigscience/bloom-2b5\") # 7b1\n",
    "tokenizer_2b5 = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-2b5\") # 7b1 # 1b3\n",
    "\n",
    "print(f\"Time to load model: {time.time()-current}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb28225",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer_2b5(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Beam Search\n",
    "current = time.time()\n",
    "\n",
    "output = (tokenizer_2b5.decode(model_2b5.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       num_beams=2, \n",
    "                       no_repeat_ngram_size=2,\n",
    "                       early_stopping=True\n",
    "                      )[0]))\n",
    "output = (output[(prompt.find('**Questions:**')):])\n",
    "print(output)\n",
    "\n",
    "print(f\"Time to Beam search: {time.time()-current}\")\n",
    "\n",
    "\n",
    "output_text.append('2b5 beam')\n",
    "output_text.append('\\n')\n",
    "output_text.append(output)\n",
    "output_text.append('Time to beam search:')\n",
    "output_text.append(time.time()-current)\n",
    "output_text.append('\\n')\n",
    "\n",
    "\n",
    "# Sampling Top-k + Top-p\n",
    "current = time.time()\n",
    "\n",
    "output = (tokenizer_2b5.decode(model_2b5.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.9\n",
    "                      )[0]))\n",
    "output = (output[(prompt.find('**Questions:**')):])\n",
    "print(output)\n",
    "print(f\"Time to Top-k + Top-p search: {time.time()-current}\")\n",
    "\n",
    "output_text.append('2b5 topk')\n",
    "output_text.append('\\n')\n",
    "output_text.append(output)\n",
    "output_text.append('Time to topk search:')\n",
    "output_text.append(time.time()-current)\n",
    "output_text.append('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f1e9ea",
   "metadata": {},
   "source": [
    "# 3b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51175ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "current = time.time()\n",
    "\n",
    "model_3b = BloomForCausalLM.from_pretrained(\"bigscience/bloom-3b\") # 7b1\n",
    "tokenizer_3b = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-3b\") # 7b1 # 1b3\n",
    "\n",
    "print(f\"Time to load model: {time.time()-current}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer_3b(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Beam Search\n",
    "current = time.time()\n",
    "\n",
    "output = (tokenizer_3b.decode(model_3b.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       num_beams=2, \n",
    "                       no_repeat_ngram_size=2,\n",
    "                       early_stopping=True\n",
    "                      )[0]))\n",
    "output = (output[(prompt.find('**Questions:**')):])\n",
    "print(output)\n",
    "print(f\"Time to Beam search: {time.time()-current}\")\n",
    "\n",
    "output_text.append('3b beam')\n",
    "output_text.append('\\n')\n",
    "output_text.append(output)\n",
    "output_text.append('Time to beam search:')\n",
    "output_text.append(time.time()-current)\n",
    "output_text.append('\\n')\n",
    "\n",
    "# Sampling Top-k + Top-p\n",
    "current = time.time()\n",
    "\n",
    "output = (tokenizer_3b.decode(model_3b.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.9\n",
    "                      )[0]))\n",
    "output = (output[(prompt.find('**Questions:**')):])\n",
    "print(output)\n",
    "print(f\"Time to Top-k + Top-p search: {time.time()-current}\")\n",
    "\n",
    "output_text.append('3b topk')\n",
    "output_text.append('\\n')\n",
    "output_text.append(output)\n",
    "output_text.append('Time to topk search:')\n",
    "output_text.append(time.time()-current)\n",
    "output_text.append('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c410a",
   "metadata": {},
   "source": [
    "# 6b3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da73647",
   "metadata": {},
   "source": [
    "current = time.time()\n",
    "\n",
    "model_6b3 = BloomForCausalLM.from_pretrained(\"bigscience/bloom-6b3\") # 7b1\n",
    "tokenizer_6b3 = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-6b3\") # 7b1 # 1b3\n",
    "\n",
    "print(f\"Time to load model: {time.time()-current}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d40f750",
   "metadata": {},
   "source": [
    "inputs = tokenizer_6b3(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f952660",
   "metadata": {},
   "source": [
    "\n",
    "########### Beam Search\n",
    "current = time.time()\n",
    "\n",
    "output = (tokenizer_6b3.decode(model_6b3.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       num_beams=2, \n",
    "                       no_repeat_ngram_size=2,\n",
    "                       early_stopping=True\n",
    "                      )[0]))\n",
    "print(output[(prompt.find('**Questions:**')):])\n",
    "print(f\"Time to Beam search: {time.time()-current}\")\n",
    "\n",
    "output_text.append('6b3 beam')\n",
    "output_text.append('\\n')\n",
    "output_text.append(output)\n",
    "output_text.append('Time to beam search:')\n",
    "output_text.append(time.time()-current)\n",
    "output_text.append('\\n')\n",
    "\n",
    "############## Sampling Top-k + Top-p\n",
    "current = time.time()\n",
    "\n",
    "output = (tokenizer_6b3.decode(model_6b3.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.9\n",
    "                      )[0]))\n",
    "print(output[(prompt.find('**Questions:**')):])\n",
    "print(f\"Time to Top-k + Top-p search: {time.time()-current}\")\n",
    "\n",
    "output_text.append('6b3 topk')\n",
    "output_text.append('\\n')\n",
    "output_text.append(output)\n",
    "output_text.append('Time to topk search:')\n",
    "output_text.append(time.time()-current)\n",
    "output_text.append('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd1e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:/Users/Declan/Desktop/MDSI/2022/Spring/iLab 2/Project/example 7 bloom.doc', 'w') as fp:\n",
    "    for item in output_text:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04661d37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
